{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-18 17:34:35 [config.py:2610] Downcasting torch.float32 to torch.float16.\n",
      "INFO 04-18 17:34:41 [config.py:585] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 04-18 17:34:41 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 04-18 17:34:43 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='d1shs0ap/cognitive-behaviors-Llama-3.2-3B', speculative_config=None, tokenizer='d1shs0ap/cognitive-behaviors-Llama-3.2-3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=d1shs0ap/cognitive-behaviors-Llama-3.2-3B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-18 17:34:43 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7900a9efd6d0>\n",
      "INFO 04-18 17:34:45 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-18 17:34:45 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-18 17:34:45 [gpu_model_runner.py:1174] Starting to load model d1shs0ap/cognitive-behaviors-Llama-3.2-3B...\n",
      "WARNING 04-18 17:34:45 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 04-18 17:34:45 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:00,  2.78it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 04-18 17:34:46 [core.py:343] EngineCore hit an exception: Traceback (most recent call last):\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 335, in run_engine_core\n",
      "ERROR 04-18 17:34:46 [core.py:343]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 04-18 17:34:46 [core.py:343]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 290, in __init__\n",
      "ERROR 04-18 17:34:46 [core.py:343]     super().__init__(vllm_config, executor_class, log_stats)\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 60, in __init__\n",
      "ERROR 04-18 17:34:46 [core.py:343]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 04-18 17:34:46 [core.py:343]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 04-18 17:34:46 [core.py:343]     self._init_executor()\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 04-18 17:34:46 [core.py:343]     self.collective_rpc(\"load_model\")\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 04-18 17:34:46 [core.py:343]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 04-18 17:34:46 [core.py:343]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/utils.py\", line 2255, in run_method\n",
      "ERROR 04-18 17:34:46 [core.py:343]     return func(*args, **kwargs)\n",
      "ERROR 04-18 17:34:46 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 136, in load_model\n",
      "ERROR 04-18 17:34:46 [core.py:343]     self.model_runner.load_model()\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1177, in load_model\n",
      "ERROR 04-18 17:34:46 [core.py:343]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 04-18 17:34:46 [core.py:343]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 04-18 17:34:46 [core.py:343]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 04-18 17:34:46 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 444, in load_model\n",
      "ERROR 04-18 17:34:46 [core.py:343]     loaded_weights = model.load_weights(\n",
      "ERROR 04-18 17:34:46 [core.py:343]                      ^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 554, in load_weights\n",
      "ERROR 04-18 17:34:46 [core.py:343]     return loader.load_weights(\n",
      "ERROR 04-18 17:34:46 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 235, in load_weights\n",
      "ERROR 04-18 17:34:46 [core.py:343]     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n",
      "ERROR 04-18 17:34:46 [core.py:343]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-18 17:34:46 [core.py:343]   File \"/home/cmu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 224, in _load_module\n",
      "ERROR 04-18 17:34:46 [core.py:343]     raise ValueError(msg)\n",
      "ERROR 04-18 17:34:46 [core.py:343] ValueError: There is no module or parameter named 'embed_tokens' in LlamaForCausalLM\n",
      "ERROR 04-18 17:34:46 [core.py:343] \n",
      "CRITICAL 04-18 17:34:46 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.54it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\"d1shs0ap/cognitive-behaviors-Llama-3.2-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "User: Using the numbers [3, 74, 25], create an equation that equals 52. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\n",
    "Assistant: Let me solve this step by step.\n",
    "<think>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(prompts=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
